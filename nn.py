# -*- coding: utf-8 -*-
"""NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18cJTb-NArmAqDDJynrz-unQuTURFBBoy

ref https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html
"""

import numpy as np 
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sn
import random
import matplotlib.pyplot as plt
import contractions
import nltk
import re
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset
from torch import nn
import torch
import time


from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""use gpu"""

!nvidia-smi

use_gpu = torch.cuda.is_available()
print(use_gpu)

"""# **text preprocessing**"""

def get_detaset():
    train_df = pd.read_csv('/content/drive/MyDrive/IFT6390/kaggle2/train_data.csv')
    test_df = pd.read_csv('/content/drive/MyDrive/IFT6390/kaggle2/test_data.csv')
    train_result_df = pd.read_csv('/content/drive/MyDrive/IFT6390/kaggle2/train_results.csv')
    return train_df, test_df, train_result_df

def treat_detaset(train_df, test_df, train_result_df):
    train_df= train_df.drop(columns=['id'])
    test_df = test_df.drop(columns=['id'])
    train_result_df = train_result_df.drop(columns=['id'])

    train_result_df.loc[train_result_df['target'] == 'negative'] = 0
    train_result_df.loc[train_result_df['target'] == 'neutral'] = 1
    train_result_df.loc[train_result_df['target'] == 'positive'] = 2
    return train_df, test_df, train_result_df

train_df, test_df, train_result_df = get_detaset()
train_df, test_df, train_result_df = treat_detaset(train_df, test_df, train_result_df)

nltk.download("wordnet")
nltk.download('omw-1.4')
lemmatizer = WordNetLemmatizer()

def text_preprocessing(df):
    #1) Expand contractions in Text Processing
    df['reviews_text']=df['text'].apply(lambda x:contractions.fix(x, slang=True))
    #2) Lower Case
    df['reviews_text'] = df['reviews_text'].str.lower()
    #3) Remove punctuations
    df['reviews_text'] = df['reviews_text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))
    #4) Remove words containing digits
    df['reviews_text'] = df['reviews_text'].apply(lambda x: re.sub(r'\w*\d\w*', '', x))
    #5) Remove Stopwords
    #def remove_stopwords(text):
    #    return " ".join([word for word in str(text).split() if word not in stop_words])
    #df['reviews_text'] = df['reviews_text'].apply(lambda x: remove_stopwords(x))
    #6) Lemmatization
    def lemmatize_words(text):
        return " ".join([lemmatizer.lemmatize(word) for word in text.split()])
    df['reviews_text'] = df['reviews_text'].apply(lambda text: lemmatize_words(text))
    #7) Remove Extra Spaces
    df['reviews_text'] = df['reviews_text'].apply(lambda x: re.sub(' +', ' ', x))
    return df

train_df_pre = text_preprocessing(train_df)
test_df_pre = text_preprocessing(test_df)

"""Access to the raw dataset iterators """

train_target_iter = iter(train_result_df.target)
train_text_iter = iter(train_df_pre.reviews_text)
#first trans dataframe to iter type dataset
train_iter = zip(train_target_iter,train_text_iter)
test_iter = iter(test_df_pre.reviews_text)

#second tran iter type dataset to map style data_set
#map style dataset can be reuse
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

"""# Prepare data processing pipelines"""

tokenizer = get_tokenizer('basic_english')
def yield_tokens(data_iter):
    for _,text in data_iter:
        yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

text_pipeline = lambda x: vocab(tokenizer(x))
#label_pipeline = lambda x: int(x) - 1

"""# Generate data batch and iterator"""

#use gpu if possible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#for train set
def collate_batch(batch):
    label_list, text_list, offsets = [], [], [0]
    for (_label, _text) in batch:
         label_list.append(_label)
         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
         text_list.append(processed_text)
         offsets.append(processed_text.size(0))
    label_list = torch.tensor(label_list, dtype=torch.int64)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    text_list = torch.cat(text_list)
    return label_list.to(device), text_list.to(device), offsets.to(device)

#for test set
def collate_batch_fortest(batch):
    text_list, offsets = [], [0]
    for _text in batch:
         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
         text_list.append(processed_text)
         offsets.append(processed_text.size(0))
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    text_list = torch.cat(text_list)
    return text_list.to(device), offsets.to(device)

"""# Define the model NN"""

class TextClassificationModel(nn.Module):

    def __init__(self, vocab_size, embed_dim, num_class):
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)
        self.fc = nn.Linear(embed_dim, num_class)
        self.init_weights()

    def init_weights(self):
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        self.fc.bias.data.zero_()

    def forward(self, text, offsets):
        embedded = self.embedding(text, offsets)
        
        return self.fc(embedded)

"""# Initiate an instance for NN"""

#for NN
num_class = 3
vocab_size = len(vocab)
emsize = 64
model = TextClassificationModel(vocab_size, emsize, num_class).to(device)

"""# Define functions to train the model and evaluate results."""

def train(dataloader):
    model.train()
    total_acc, total_count = 0, 0
    log_interval = 500
    start_time = time.time()

    for idx, (label, text, offsets) in enumerate(dataloader):
        optimizer.zero_grad()
        predicted_label = model(text, offsets)

        #print(len(predicted_label))
        #print(len(label))
        #print(label)
        #print(predicted_label)

        loss = criterion(predicted_label, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
        optimizer.step()
        total_acc += (predicted_label.argmax(1) == label).sum().item()
        total_count += label.size(0)
        if idx % log_interval == 0 and idx > 0:
            elapsed = time.time() - start_time
            print('| epoch {:3d} | {:5d}/{:5d} batches '
                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),
                                              total_acc/total_count))
            total_acc, total_count = 0, 0
            start_time = time.time()

def evaluate(dataloader):
    model.eval()
    total_acc, total_count = 0, 0

    with torch.no_grad():
        for idx, (label, text, offsets) in enumerate(dataloader):
            predicted_label = model(text, offsets)
            loss = criterion(predicted_label, label)
            total_acc += (predicted_label.argmax(1) == label).sum().item()
            total_count += label.size(0)
    return total_acc/total_count

"""# Hyperparameters"""

# Hyperparameters
EPOCHS = 15 # epoch
LR = 0.1  # learning rate
BATCH_SIZE = 64 # batch size for training

criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)

"""# Split the dataset and run the model"""

#split train valid set
num_train = int(len(train_dataset) * 0.80)
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])
#get pytorch dataloader 
train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,
                              shuffle=True, collate_fn=collate_batch)
valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,
                              shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(test_dataset, shuffle=False, collate_fn=collate_batch_fortest)

"""# run the model NN"""

total_accu = 0
for epoch in range(1, EPOCHS + 1):
    epoch_start_time = time.time()
    train(train_dataloader)
    accu_val = evaluate(valid_dataloader)
    if total_accu is not None and total_accu > accu_val:
      scheduler.step()
    else:
       total_accu = accu_val
    print('-' * 59)
    print('| end of epoch {:3d} | time: {:5.2f}s | '
          'valid accuracy {:8.3f} '.format(epoch,
                                           time.time() - epoch_start_time,
                                           accu_val))
    print('-' * 59)

def pred_test(dataloader):
    pred_labels_list = []
    total_acc, total_count = 0, 0

    with torch.no_grad():
        for idx, (text, offsets) in tqdm(enumerate(dataloader)):
            #print(idx)
            predicted_label = model(text, offsets)
            pred_labels_list.append(predicted_label.argmax(1))
    pred_labels = torch.stack(pred_labels_list, 0)
    return pred_labels

pred_labels = pred_test(test_dataloader)

df_submit = pd.DataFrame(pred_labels, columns = ['target'])
df_submit = df_submit.reset_index()
df_submit['target'] = df_submit['target'].astype("int")
df_submit.rename(columns={'index':'id'}, inplace = True)

df_submit.to_csv('./NN1.csv', index = False)

"""# Try RNN"""

class RNN(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):
        super().__init__()        
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)     
        self.rnn = nn.RNN(embed_dim, hidden_dim)       
        self.fc = nn.Linear(hidden_dim, output_dim)

    def init_weights(self):
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        self.fc.bias.data.zero_()
        
    def forward(self, text, offsets):
        #text = [sent len, batch size]       
        embedded = self.embedding(text, offsets)      
        #print(embedded.shape)
        #embedded = [sent len, batch size, emb dim]        
        output, hidden = self.rnn(embedded)
        #print(output.shape)    
        #print(hidden.shape)  
        #output = [sent len, batch size, hid dim]
        #hidden = [1, batch size, hid dim]      
        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))# [1, batch size, hid dim] -> [batch size, hid dim]
        
        return self.fc(output)

#rnn

INPUT_DIM = len(vocab)
EMBEDDING_DIM = 64
HIDDEN_DIM = 256
OUTPUT_DIM = 3

model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)

# Hyperparameters
EPOCHS = 5 # epoch
LR = 5  # learning rate
BATCH_SIZE = 64 # batch size for training

criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)

#split train valid set
num_train = int(len(train_dataset) * 0.80)
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])
#get pytorch dataloader 
train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,
                              shuffle=True, collate_fn=collate_batch)
valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,
                              shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(test_dataset, shuffle=False, collate_fn=collate_batch_fortest)

print(len(split_train_))
print(len(split_valid_))
len(train_dataloader)

total_accu = 0
for epoch in range(1, EPOCHS + 1):
    epoch_start_time = time.time()
    train(train_dataloader)
    accu_val = evaluate(valid_dataloader)
    if total_accu is not None and total_accu > accu_val:
      scheduler.step()
    else:
       total_accu = accu_val
    print('-' * 59)
    print('| end of epoch {:3d} | time: {:5.2f}s | '
          'valid accuracy {:8.3f} '.format(epoch,
                                           time.time() - epoch_start_time,
                                           accu_val))
    print('-' * 59)