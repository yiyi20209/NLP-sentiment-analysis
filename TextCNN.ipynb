{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mM2iUCmDnl3W","executionInfo":{"status":"ok","timestamp":1671323097988,"user_tz":300,"elapsed":2573,"user":{"displayName":"linrui ma","userId":"10315317965388359510"}},"outputId":"77254a0b-8c99-4d18-de1d-07d6fe64e4d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec 18 00:27:40 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    25W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lHN9SsZkXGN"},"outputs":[],"source":["import gensim\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","from collections import Counter\n","from torch.utils.data import TensorDataset,DataLoader\n","from torch.optim.lr_scheduler import *"]},{"cell_type":"code","source":["from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYlU8NTbodjz","executionInfo":{"status":"ok","timestamp":1671324165503,"user_tz":300,"elapsed":2457,"user":{"displayName":"linrui ma","userId":"10315317965388359510"}},"outputId":"b9045fbd-3ebf-436a-caa6-35d3776044fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["Word_Vector_path = '/content/drive/MyDrive/Dataset/data.vector'\n","Train_Ro_path = '/content/drive/MyDrive/Dataset/train_ro.txt'\n","Valid_Ro_path = '/content/drive/MyDrive/Dataset/valid_ro.txt'\n","\n","learning_rate = 0.001  \n","BATCH_SIZE = 64 \n","EPOCHS = 5 \n","model_path = None  # path of pretrain model"],"metadata":{"id":"PSNntvUpkgL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_word2id(file, save_to_path=None):\n","    \"\"\"\n","    :param file: word2id save path\n","    :param save_to_path: save word2vec\n","    :return: None\n","    \"\"\"\n","    word2id = {'_PAD_': 0}\n","    path = ['/content/drive/MyDrive/Dataset/train.txt', '/content/drive/MyDrive/Dataset/validation.txt']\n","\n","    for _path in path:\n","        with open(_path, encoding='utf-8') as f:\n","            for line in f.readlines():\n","                sp = line.strip().split()\n","                for word in sp[1:]:\n","                    if word not in word2id.keys():\n","                        word2id[word] = len(word2id)\n","    if save_to_path:\n","        with open(file, 'w', encoding='utf-8') as f:\n","            for w in word2id:\n","                f.write(w + '\\t')\n","                f.write(str(word2id[w]))\n","                f.write('\\n')\n","\n","    return word2id"],"metadata":{"id":"eVr_Mgv2knQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_word2vec(fname, word2id, save_to_path=None):\n","    \"\"\"\n","    :param fname: pretrain word2vec.\n","    :param word2id: the vocabulary set of all word in document.\n","    :param save_to_path: save word2vec to local\n","    :return: word2vec vector corresponding to the vocabulary set {id: word2vec}.\n","    \"\"\"\n","    n_words = max(word2id.values()) + 1\n","    model = gensim.models.KeyedVectors.load_word2vec_format(fname)\n","    word_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size]))\n","    for word in word2id.keys():\n","        try:\n","            word_vecs[word2id[word]] = model[word]\n","        except KeyError:\n","            pass\n","    if save_to_path:\n","        with open(save_to_path, 'w', encoding='utf-8') as f:\n","            for vec in word_vecs:\n","                vec = [str(w) for w in vec]\n","                f.write(' '.join(vec))\n","                f.write('\\n')\n","    return word_vecs\n","\n","def cat_to_id(classes=None):\n","    \"\"\"\n","    :param classes: label class\n","    :return: {class：id}\n","    \"\"\"\n","    if not classes:\n","        classes = ['0', '1', '2']\n","    cat2id = {cat: idx for (idx, cat) in enumerate(classes)}\n","    return classes, cat2id\n","\n","def load_corpus(path, word2id, max_sen_len=50):\n","    \"\"\"\n","    :param path: data path\n","    :return: contents，labels(onehot)\n","    \"\"\"\n","    _, cat2id = cat_to_id()\n","    contents, labels = [], []\n","    with open(path, encoding='utf-8') as f:\n","        for line in f.readlines():\n","            sp = line.strip().split()\n","            label = sp[0]\n","            content = [word2id.get(w, 0) for w in sp[1:]]\n","            content = content[:max_sen_len]\n","            if len(content) < max_sen_len:\n","                content += [word2id['_PAD_']] * (max_sen_len - len(content))\n","            labels.append(label)\n","            contents.append(content)\n","    counter = Counter(labels)\n","    print('Total sample num：%d' % (len(labels)))\n","    print('class num：')\n","    for w in counter:\n","        print(w, counter[w])\n","\n","    contents = np.asarray(contents)\n","    labels = np.array([cat2id[l] for l in labels])\n","\n","    return contents, labels\n","\n","def load_corpus(path, word2id, max_sen_len=50):\n","    \"\"\"\n","    :param path: data path\n","    :return: contents，labels(onehot)\n","    \"\"\"\n","    _, cat2id = cat_to_id()\n","    contents, labels = [], []\n","    with open(path, encoding='utf-8') as f:\n","        for line in f.readlines():\n","            sp = line.strip().split()\n","            label = sp[0]\n","            content = [word2id.get(w, 0) for w in sp[1:]]\n","            content = content[:max_sen_len]\n","            if len(content) < max_sen_len:\n","                content += [word2id['_PAD_']] * (max_sen_len - len(content))\n","            labels.append(label)\n","            contents.append(content)\n","    counter = Counter(labels)\n","    print('Total sample num：%d' % (len(labels)))\n","    print('class num：')\n","    for w in counter:\n","        print(w, counter[w])\n","\n","    contents = np.asarray(contents)\n","    labels = np.array([cat2id[l] for l in labels])\n","\n","    return contents, labels"],"metadata":{"id":"Ah8Y_FvRkp3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word2id = build_word2id('./Dataset/word2id.txt')\n","# print(word2id)\n","word2vec = build_word2vec(Word_Vector_path, word2id)\n","print(word2vec.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J7vQDzqhkzAB","executionInfo":{"status":"ok","timestamp":1671324193235,"user_tz":300,"elapsed":12404,"user":{"displayName":"linrui ma","userId":"10315317965388359510"}},"outputId":"6a19ee5c-a883-4be0-a857-aba42a32ce65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(449913, 100)\n"]}]},{"cell_type":"code","source":["class CONFIG():\n","    update_w2v = True  # update word2vec when training\n","    vocab_size = word2vec.shape[0]  # number of word，same to word2id\n","    n_class = 3  # pos neg neu\n","    embedding_dim = 100  \n","    drop_keep_prob = 0.5  # dropout parametre\n","    kernel_num = 64  # number of filter in conv layer\n","    kernel_size = [3, 4, 5]  # dimension of conv kernel\n","    pretrained_embed = word2vec  # our mode"],"metadata":{"id":"YlweFCJVk2h3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextCNN(nn.Module):\n","    def __init__(self, config):\n","        super(TextCNN, self).__init__()\n","        update_w2v = config.update_w2v\n","        vocab_size = config.vocab_size\n","        n_class = config.n_class\n","        embedding_dim = config.embedding_dim\n","        kernel_num = config.kernel_num\n","        kernel_size = config.kernel_size\n","        drop_keep_prob = config.drop_keep_prob\n","        pretrained_embed = config.pretrained_embed\n","\n","        # use word2vec\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embed))\n","        self.embedding.weight.requires_grad = update_w2v\n","        # conv\n","        self.conv1 = nn.Conv2d(1, kernel_num, (kernel_size[0], embedding_dim))\n","        self.conv2 = nn.Conv2d(1, kernel_num, (kernel_size[1], embedding_dim))\n","        self.conv3 = nn.Conv2d(1, kernel_num, (kernel_size[2], embedding_dim))\n","        # Dropout\n","        self.dropout = nn.Dropout(drop_keep_prob)\n","        # linear\n","        self.fc = nn.Linear(len(kernel_size) * kernel_num, n_class)\n","\n","    @staticmethod\n","    def conv_and_pool(x, conv):\n","        # x: (batch, 1, sentence_length,  )\n","        x = conv(x)\n","        # x: (batch, kernel_num, H_out, 1)\n","        x = F.relu(x.squeeze(3))\n","        # x: (batch, kernel_num, H_out)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        #  (batch, kernel_num)\n","        return x\n","\n","    def forward(self, x):\n","        x = x.to(torch.int64)\n","        x = self.embedding(x)\n","        x = x.unsqueeze(1)\n","        x1 = self.conv_and_pool(x, self.conv1)  # (batch, kernel_num)\n","        x2 = self.conv_and_pool(x, self.conv2)  # (batch, kernel_num)\n","        x3 = self.conv_and_pool(x, self.conv3)  # (batch, kernel_num)\n","        x = torch.cat((x1, x2, x3), 1)  # (batch, 3 * kernel_num)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        x = F.log_softmax(x, dim=1)\n","        return x"],"metadata":{"id":"_HtJMeX4k3VY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(dataloader, epoch):\n","\n","    train_loss, train_acc = 0.0, 0.0\n","    count, correct = 0, 0\n","    for batch_idx, (x, y) in enumerate(dataloader):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = criterion(output, y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        correct += (output.argmax(1) == y).float().sum().item()\n","        count += len(x)\n","\n","        if (batch_idx + 1) % 100 == 0:\n","            print('train epoch: {} [{}/{} ({:.0f}%)]\\tloss: {:.6f}'.format(\n","                epoch, batch_idx * len(x), len(dataloader.dataset),\n","                       100. * batch_idx / len(dataloader), loss.item()))\n","\n","    train_loss *= BATCH_SIZE\n","    train_loss /= len(dataloader.dataset)\n","    train_acc = correct / count\n","    print('\\ntrain epoch: {}\\taverage loss: {:.6f}\\taccuracy:{:.4f}%\\n'.format(epoch, train_loss, 100. * train_acc))\n","    scheduler.step()\n","\n","    return train_loss, train_acc\n","\n","def validation(dataloader, epoch):\n","    model.eval()\n","    # valid\n","    val_loss, val_acc = 0.0, 0.0\n","    count, correct = 0, 0\n","    for _, (x, y) in enumerate(dataloader):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        output = model(x)\n","        loss = criterion(output, y)\n","        val_loss += loss.item()\n","        correct += (output.argmax(1) == y).float().sum().item()\n","        count += len(x)\n","\n","    val_loss *= BATCH_SIZE\n","    val_loss /= len(dataloader.dataset)\n","    val_acc = correct / count\n","    # print acc\n","    print(\n","        'validation:train epoch: {}\\taverage loss: {:.6f}\\t accuracy:{:.2f}%\\n'.format(epoch, val_loss, 100 * val_acc))\n","\n","    return val_loss, val_acc"],"metadata":{"id":"QeU7OoExk5Xk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('train set: ')\n","train_contents, train_labels = load_corpus(Train_Ro_path, word2id, max_sen_len=100)\n","print('\\nvalidation set: ')\n","val_contents, val_labels = load_corpus(Valid_Ro_path, word2id, max_sen_len=100)\n","\n","config = CONFIG()  \n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","train_dataset = TensorDataset(torch.from_numpy(train_contents).type(torch.float),\n","                              torch.from_numpy(train_labels).type(torch.long))\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n","                              shuffle=True, num_workers=2)\n","\n","val_dataset = TensorDataset(torch.from_numpy(val_contents).type(torch.float),\n","                            torch.from_numpy(val_labels).type(torch.long))\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE,\n","                            shuffle=True, num_workers=2)"],"metadata":{"id":"YjrwMsbLk8Kk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671324236998,"user_tz":300,"elapsed":26521,"user":{"displayName":"linrui ma","userId":"10315317965388359510"}},"outputId":"4e5f9e14-aae4-4576-a67d-4d7a287e4b99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train set: \n","Total sample num：900000\n","class num：\n","2 450556\n","0 449369\n","1 75\n","\n","validation set: \n","Total sample num：140323\n","class num：\n","2 69880\n","0 70434\n","1 9\n"]}]},{"cell_type":"code","source":["# model, continue last training\n","model = TextCNN(config)\n","if model_path:\n","    model.load_state_dict(torch.load(model_path))\n","model.to(DEVICE)\n","\n","# optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# loss func\n","criterion = nn.CrossEntropyLoss()\n","scheduler = StepLR(optimizer, step_size=5)"],"metadata":{"id":"n4ChBkewk-BT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_losses = []\n","train_acces = []\n","val_losses = []\n","val_acces = []\n","\n","for epoch in range(1, EPOCHS + 1):\n","    tr_loss, tr_acc = train(train_dataloader, epoch)\n","    val_loss, val_acc = validation(val_dataloader, epoch)\n","    train_losses.append(tr_loss)\n","    train_acces.append(tr_acc)\n","    val_losses.append(val_loss)\n","    val_acces.append(val_acc)\n","\n","model_pth = 'model_' + str(time.time()) + '.pth'\n","torch.save(model.state_dict(), model_pth)"],"metadata":{"id":"7I7kDNUinBOd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671327547823,"user_tz":300,"elapsed":3297525,"user":{"displayName":"linrui ma","userId":"10315317965388359510"}},"outputId":"6ed3bf12-1a6f-4807-9f45-97a6bda830c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train epoch: 1 [6336/900000 (1%)]\tloss: 0.530537\n","train epoch: 1 [12736/900000 (1%)]\tloss: 0.530369\n","train epoch: 1 [19136/900000 (2%)]\tloss: 0.504092\n","train epoch: 1 [25536/900000 (3%)]\tloss: 0.423896\n","train epoch: 1 [31936/900000 (4%)]\tloss: 0.461720\n","train epoch: 1 [38336/900000 (4%)]\tloss: 0.425118\n","train epoch: 1 [44736/900000 (5%)]\tloss: 0.643242\n","train epoch: 1 [51136/900000 (6%)]\tloss: 0.373954\n","train epoch: 1 [57536/900000 (6%)]\tloss: 0.425403\n","train epoch: 1 [63936/900000 (7%)]\tloss: 0.381689\n","train epoch: 1 [70336/900000 (8%)]\tloss: 0.410549\n","train epoch: 1 [76736/900000 (9%)]\tloss: 0.486028\n","train epoch: 1 [83136/900000 (9%)]\tloss: 0.492174\n","train epoch: 1 [89536/900000 (10%)]\tloss: 0.435865\n","train epoch: 1 [95936/900000 (11%)]\tloss: 0.434065\n","train epoch: 1 [102336/900000 (11%)]\tloss: 0.572894\n","train epoch: 1 [108736/900000 (12%)]\tloss: 0.442665\n","train epoch: 1 [115136/900000 (13%)]\tloss: 0.476722\n","train epoch: 1 [121536/900000 (14%)]\tloss: 0.368249\n","train epoch: 1 [127936/900000 (14%)]\tloss: 0.482272\n","train epoch: 1 [134336/900000 (15%)]\tloss: 0.445504\n","train epoch: 1 [140736/900000 (16%)]\tloss: 0.448695\n","train epoch: 1 [147136/900000 (16%)]\tloss: 0.521557\n","train epoch: 1 [153536/900000 (17%)]\tloss: 0.683713\n","train epoch: 1 [159936/900000 (18%)]\tloss: 0.461227\n","train epoch: 1 [166336/900000 (18%)]\tloss: 0.371573\n","train epoch: 1 [172736/900000 (19%)]\tloss: 0.553343\n","train epoch: 1 [179136/900000 (20%)]\tloss: 0.512825\n","train epoch: 1 [185536/900000 (21%)]\tloss: 0.419101\n","train epoch: 1 [191936/900000 (21%)]\tloss: 0.440616\n","train epoch: 1 [198336/900000 (22%)]\tloss: 0.367001\n","train epoch: 1 [204736/900000 (23%)]\tloss: 0.391182\n","train epoch: 1 [211136/900000 (23%)]\tloss: 0.612962\n","train epoch: 1 [217536/900000 (24%)]\tloss: 0.484168\n","train epoch: 1 [223936/900000 (25%)]\tloss: 0.341825\n","train epoch: 1 [230336/900000 (26%)]\tloss: 0.497207\n","train epoch: 1 [236736/900000 (26%)]\tloss: 0.476017\n","train epoch: 1 [243136/900000 (27%)]\tloss: 0.372175\n","train epoch: 1 [249536/900000 (28%)]\tloss: 0.406559\n","train epoch: 1 [255936/900000 (28%)]\tloss: 0.329360\n","train epoch: 1 [262336/900000 (29%)]\tloss: 0.443532\n","train epoch: 1 [268736/900000 (30%)]\tloss: 0.588049\n","train epoch: 1 [275136/900000 (31%)]\tloss: 0.407675\n","train epoch: 1 [281536/900000 (31%)]\tloss: 0.372252\n","train epoch: 1 [287936/900000 (32%)]\tloss: 0.391094\n","train epoch: 1 [294336/900000 (33%)]\tloss: 0.425611\n","train epoch: 1 [300736/900000 (33%)]\tloss: 0.456422\n","train epoch: 1 [307136/900000 (34%)]\tloss: 0.445651\n","train epoch: 1 [313536/900000 (35%)]\tloss: 0.434175\n","train epoch: 1 [319936/900000 (36%)]\tloss: 0.517284\n","train epoch: 1 [326336/900000 (36%)]\tloss: 0.463348\n","train epoch: 1 [332736/900000 (37%)]\tloss: 0.508025\n","train epoch: 1 [339136/900000 (38%)]\tloss: 0.518797\n","train epoch: 1 [345536/900000 (38%)]\tloss: 0.584352\n","train epoch: 1 [351936/900000 (39%)]\tloss: 0.395685\n","train epoch: 1 [358336/900000 (40%)]\tloss: 0.600944\n","train epoch: 1 [364736/900000 (41%)]\tloss: 0.353752\n","train epoch: 1 [371136/900000 (41%)]\tloss: 0.466592\n","train epoch: 1 [377536/900000 (42%)]\tloss: 0.444211\n","train epoch: 1 [383936/900000 (43%)]\tloss: 0.456155\n","train epoch: 1 [390336/900000 (43%)]\tloss: 0.453328\n","train epoch: 1 [396736/900000 (44%)]\tloss: 0.472639\n","train epoch: 1 [403136/900000 (45%)]\tloss: 0.485934\n","train epoch: 1 [409536/900000 (46%)]\tloss: 0.532910\n","train epoch: 1 [415936/900000 (46%)]\tloss: 0.564603\n","train epoch: 1 [422336/900000 (47%)]\tloss: 0.425447\n","train epoch: 1 [428736/900000 (48%)]\tloss: 0.461423\n","train epoch: 1 [435136/900000 (48%)]\tloss: 0.446791\n","train epoch: 1 [441536/900000 (49%)]\tloss: 0.486647\n","train epoch: 1 [447936/900000 (50%)]\tloss: 0.402469\n","train epoch: 1 [454336/900000 (50%)]\tloss: 0.449201\n","train epoch: 1 [460736/900000 (51%)]\tloss: 0.386118\n","train epoch: 1 [467136/900000 (52%)]\tloss: 0.468690\n","train epoch: 1 [473536/900000 (53%)]\tloss: 0.391977\n","train epoch: 1 [479936/900000 (53%)]\tloss: 0.493349\n","train epoch: 1 [486336/900000 (54%)]\tloss: 0.536718\n","train epoch: 1 [492736/900000 (55%)]\tloss: 0.332322\n","train epoch: 1 [499136/900000 (55%)]\tloss: 0.527004\n","train epoch: 1 [505536/900000 (56%)]\tloss: 0.291852\n","train epoch: 1 [511936/900000 (57%)]\tloss: 0.548540\n","train epoch: 1 [518336/900000 (58%)]\tloss: 0.497439\n","train epoch: 1 [524736/900000 (58%)]\tloss: 0.381894\n","train epoch: 1 [531136/900000 (59%)]\tloss: 0.413534\n","train epoch: 1 [537536/900000 (60%)]\tloss: 0.388202\n","train epoch: 1 [543936/900000 (60%)]\tloss: 0.472434\n","train epoch: 1 [550336/900000 (61%)]\tloss: 0.299673\n","train epoch: 1 [556736/900000 (62%)]\tloss: 0.400198\n","train epoch: 1 [563136/900000 (63%)]\tloss: 0.420064\n","train epoch: 1 [569536/900000 (63%)]\tloss: 0.526524\n","train epoch: 1 [575936/900000 (64%)]\tloss: 0.480066\n","train epoch: 1 [582336/900000 (65%)]\tloss: 0.364144\n","train epoch: 1 [588736/900000 (65%)]\tloss: 0.283841\n","train epoch: 1 [595136/900000 (66%)]\tloss: 0.417743\n","train epoch: 1 [601536/900000 (67%)]\tloss: 0.281110\n","train epoch: 1 [607936/900000 (68%)]\tloss: 0.398733\n","train epoch: 1 [614336/900000 (68%)]\tloss: 0.397481\n","train epoch: 1 [620736/900000 (69%)]\tloss: 0.526063\n","train epoch: 1 [627136/900000 (70%)]\tloss: 0.457847\n","train epoch: 1 [633536/900000 (70%)]\tloss: 0.607052\n","train epoch: 1 [639936/900000 (71%)]\tloss: 0.417170\n","train epoch: 1 [646336/900000 (72%)]\tloss: 0.511560\n","train epoch: 1 [652736/900000 (73%)]\tloss: 0.510010\n","train epoch: 1 [659136/900000 (73%)]\tloss: 0.425393\n","train epoch: 1 [665536/900000 (74%)]\tloss: 0.534297\n","train epoch: 1 [671936/900000 (75%)]\tloss: 0.475170\n","train epoch: 1 [678336/900000 (75%)]\tloss: 0.404800\n","train epoch: 1 [684736/900000 (76%)]\tloss: 0.378349\n","train epoch: 1 [691136/900000 (77%)]\tloss: 0.570894\n","train epoch: 1 [697536/900000 (78%)]\tloss: 0.408784\n","train epoch: 1 [703936/900000 (78%)]\tloss: 0.602005\n","train epoch: 1 [710336/900000 (79%)]\tloss: 0.457257\n","train epoch: 1 [716736/900000 (80%)]\tloss: 0.525356\n","train epoch: 1 [723136/900000 (80%)]\tloss: 0.514352\n","train epoch: 1 [729536/900000 (81%)]\tloss: 0.453765\n","train epoch: 1 [735936/900000 (82%)]\tloss: 0.504324\n","train epoch: 1 [742336/900000 (82%)]\tloss: 0.383494\n","train epoch: 1 [748736/900000 (83%)]\tloss: 0.365221\n","train epoch: 1 [755136/900000 (84%)]\tloss: 0.373419\n","train epoch: 1 [761536/900000 (85%)]\tloss: 0.425903\n","train epoch: 1 [767936/900000 (85%)]\tloss: 0.423203\n","train epoch: 1 [774336/900000 (86%)]\tloss: 0.323911\n","train epoch: 1 [780736/900000 (87%)]\tloss: 0.392803\n","train epoch: 1 [787136/900000 (87%)]\tloss: 0.438023\n","train epoch: 1 [793536/900000 (88%)]\tloss: 0.515461\n","train epoch: 1 [799936/900000 (89%)]\tloss: 0.413717\n","train epoch: 1 [806336/900000 (90%)]\tloss: 0.516918\n","train epoch: 1 [812736/900000 (90%)]\tloss: 0.515676\n","train epoch: 1 [819136/900000 (91%)]\tloss: 0.362705\n","train epoch: 1 [825536/900000 (92%)]\tloss: 0.451237\n","train epoch: 1 [831936/900000 (92%)]\tloss: 0.460062\n","train epoch: 1 [838336/900000 (93%)]\tloss: 0.560215\n","train epoch: 1 [844736/900000 (94%)]\tloss: 0.439403\n","train epoch: 1 [851136/900000 (95%)]\tloss: 0.359350\n","train epoch: 1 [857536/900000 (95%)]\tloss: 0.541537\n","train epoch: 1 [863936/900000 (96%)]\tloss: 0.446807\n","train epoch: 1 [870336/900000 (97%)]\tloss: 0.452640\n","train epoch: 1 [876736/900000 (97%)]\tloss: 0.382110\n","train epoch: 1 [883136/900000 (98%)]\tloss: 0.378200\n","train epoch: 1 [889536/900000 (99%)]\tloss: 0.331580\n","train epoch: 1 [895936/900000 (100%)]\tloss: 0.449457\n","\n","train epoch: 1\taverage loss: 0.455859\taccuracy:78.9667%\n","\n","validation:train epoch: 1\taverage loss: 0.416297\t accuracy:81.40%\n","\n","train epoch: 2 [6336/900000 (1%)]\tloss: 0.424762\n","train epoch: 2 [12736/900000 (1%)]\tloss: 0.353191\n","train epoch: 2 [19136/900000 (2%)]\tloss: 0.304412\n","train epoch: 2 [25536/900000 (3%)]\tloss: 0.385267\n","train epoch: 2 [31936/900000 (4%)]\tloss: 0.453467\n","train epoch: 2 [38336/900000 (4%)]\tloss: 0.408760\n","train epoch: 2 [44736/900000 (5%)]\tloss: 0.347275\n","train epoch: 2 [51136/900000 (6%)]\tloss: 0.468585\n","train epoch: 2 [57536/900000 (6%)]\tloss: 0.293442\n","train epoch: 2 [63936/900000 (7%)]\tloss: 0.323202\n","train epoch: 2 [70336/900000 (8%)]\tloss: 0.390497\n","train epoch: 2 [76736/900000 (9%)]\tloss: 0.370043\n","train epoch: 2 [83136/900000 (9%)]\tloss: 0.430133\n","train epoch: 2 [89536/900000 (10%)]\tloss: 0.414377\n","train epoch: 2 [95936/900000 (11%)]\tloss: 0.396786\n","train epoch: 2 [102336/900000 (11%)]\tloss: 0.504266\n","train epoch: 2 [108736/900000 (12%)]\tloss: 0.241562\n","train epoch: 2 [115136/900000 (13%)]\tloss: 0.287029\n","train epoch: 2 [121536/900000 (14%)]\tloss: 0.470248\n","train epoch: 2 [127936/900000 (14%)]\tloss: 0.467018\n","train epoch: 2 [134336/900000 (15%)]\tloss: 0.508260\n","train epoch: 2 [140736/900000 (16%)]\tloss: 0.412914\n","train epoch: 2 [147136/900000 (16%)]\tloss: 0.352645\n","train epoch: 2 [153536/900000 (17%)]\tloss: 0.384634\n","train epoch: 2 [159936/900000 (18%)]\tloss: 0.299566\n","train epoch: 2 [166336/900000 (18%)]\tloss: 0.374279\n","train epoch: 2 [172736/900000 (19%)]\tloss: 0.366949\n","train epoch: 2 [179136/900000 (20%)]\tloss: 0.369300\n","train epoch: 2 [185536/900000 (21%)]\tloss: 0.463162\n","train epoch: 2 [191936/900000 (21%)]\tloss: 0.256730\n","train epoch: 2 [198336/900000 (22%)]\tloss: 0.364338\n","train epoch: 2 [204736/900000 (23%)]\tloss: 0.425984\n","train epoch: 2 [211136/900000 (23%)]\tloss: 0.357499\n","train epoch: 2 [217536/900000 (24%)]\tloss: 0.427111\n","train epoch: 2 [223936/900000 (25%)]\tloss: 0.374366\n","train epoch: 2 [230336/900000 (26%)]\tloss: 0.362939\n","train epoch: 2 [236736/900000 (26%)]\tloss: 0.503727\n","train epoch: 2 [243136/900000 (27%)]\tloss: 0.369495\n","train epoch: 2 [249536/900000 (28%)]\tloss: 0.332626\n","train epoch: 2 [255936/900000 (28%)]\tloss: 0.276598\n","train epoch: 2 [262336/900000 (29%)]\tloss: 0.486794\n","train epoch: 2 [268736/900000 (30%)]\tloss: 0.320338\n","train epoch: 2 [275136/900000 (31%)]\tloss: 0.365516\n","train epoch: 2 [281536/900000 (31%)]\tloss: 0.399392\n","train epoch: 2 [287936/900000 (32%)]\tloss: 0.477974\n","train epoch: 2 [294336/900000 (33%)]\tloss: 0.191911\n","train epoch: 2 [300736/900000 (33%)]\tloss: 0.473214\n","train epoch: 2 [307136/900000 (34%)]\tloss: 0.327323\n","train epoch: 2 [313536/900000 (35%)]\tloss: 0.198474\n","train epoch: 2 [319936/900000 (36%)]\tloss: 0.380275\n","train epoch: 2 [326336/900000 (36%)]\tloss: 0.352733\n","train epoch: 2 [332736/900000 (37%)]\tloss: 0.450693\n","train epoch: 2 [339136/900000 (38%)]\tloss: 0.376727\n","train epoch: 2 [345536/900000 (38%)]\tloss: 0.388932\n","train epoch: 2 [351936/900000 (39%)]\tloss: 0.462035\n","train epoch: 2 [358336/900000 (40%)]\tloss: 0.520071\n","train epoch: 2 [364736/900000 (41%)]\tloss: 0.370254\n","train epoch: 2 [371136/900000 (41%)]\tloss: 0.374983\n","train epoch: 2 [377536/900000 (42%)]\tloss: 0.262351\n","train epoch: 2 [383936/900000 (43%)]\tloss: 0.451503\n","train epoch: 2 [390336/900000 (43%)]\tloss: 0.336979\n","train epoch: 2 [396736/900000 (44%)]\tloss: 0.305693\n","train epoch: 2 [403136/900000 (45%)]\tloss: 0.335409\n","train epoch: 2 [409536/900000 (46%)]\tloss: 0.425185\n","train epoch: 2 [415936/900000 (46%)]\tloss: 0.445019\n","train epoch: 2 [422336/900000 (47%)]\tloss: 0.398344\n","train epoch: 2 [428736/900000 (48%)]\tloss: 0.339659\n","train epoch: 2 [435136/900000 (48%)]\tloss: 0.367262\n","train epoch: 2 [441536/900000 (49%)]\tloss: 0.343737\n","train epoch: 2 [447936/900000 (50%)]\tloss: 0.235742\n","train epoch: 2 [454336/900000 (50%)]\tloss: 0.382545\n","train epoch: 2 [460736/900000 (51%)]\tloss: 0.367657\n","train epoch: 2 [467136/900000 (52%)]\tloss: 0.298413\n","train epoch: 2 [473536/900000 (53%)]\tloss: 0.404851\n","train epoch: 2 [479936/900000 (53%)]\tloss: 0.387617\n","train epoch: 2 [486336/900000 (54%)]\tloss: 0.379737\n","train epoch: 2 [492736/900000 (55%)]\tloss: 0.326027\n","train epoch: 2 [499136/900000 (55%)]\tloss: 0.240291\n","train epoch: 2 [505536/900000 (56%)]\tloss: 0.395142\n","train epoch: 2 [511936/900000 (57%)]\tloss: 0.446513\n","train epoch: 2 [518336/900000 (58%)]\tloss: 0.342203\n","train epoch: 2 [524736/900000 (58%)]\tloss: 0.372984\n","train epoch: 2 [531136/900000 (59%)]\tloss: 0.539876\n","train epoch: 2 [537536/900000 (60%)]\tloss: 0.357644\n","train epoch: 2 [543936/900000 (60%)]\tloss: 0.395798\n","train epoch: 2 [550336/900000 (61%)]\tloss: 0.464195\n","train epoch: 2 [556736/900000 (62%)]\tloss: 0.439390\n","train epoch: 2 [563136/900000 (63%)]\tloss: 0.393167\n","train epoch: 2 [569536/900000 (63%)]\tloss: 0.420190\n","train epoch: 2 [575936/900000 (64%)]\tloss: 0.397272\n","train epoch: 2 [582336/900000 (65%)]\tloss: 0.374527\n","train epoch: 2 [588736/900000 (65%)]\tloss: 0.333182\n","train epoch: 2 [595136/900000 (66%)]\tloss: 0.316062\n","train epoch: 2 [601536/900000 (67%)]\tloss: 0.473688\n","train epoch: 2 [607936/900000 (68%)]\tloss: 0.500182\n","train epoch: 2 [614336/900000 (68%)]\tloss: 0.373742\n","train epoch: 2 [620736/900000 (69%)]\tloss: 0.389032\n","train epoch: 2 [627136/900000 (70%)]\tloss: 0.377676\n","train epoch: 2 [633536/900000 (70%)]\tloss: 0.244698\n","train epoch: 2 [639936/900000 (71%)]\tloss: 0.353910\n","train epoch: 2 [646336/900000 (72%)]\tloss: 0.315956\n","train epoch: 2 [652736/900000 (73%)]\tloss: 0.378547\n","train epoch: 2 [659136/900000 (73%)]\tloss: 0.301279\n","train epoch: 2 [665536/900000 (74%)]\tloss: 0.403301\n","train epoch: 2 [671936/900000 (75%)]\tloss: 0.258949\n","train epoch: 2 [678336/900000 (75%)]\tloss: 0.383686\n","train epoch: 2 [684736/900000 (76%)]\tloss: 0.514887\n","train epoch: 2 [691136/900000 (77%)]\tloss: 0.386267\n","train epoch: 2 [697536/900000 (78%)]\tloss: 0.380617\n","train epoch: 2 [703936/900000 (78%)]\tloss: 0.395266\n","train epoch: 2 [710336/900000 (79%)]\tloss: 0.345226\n","train epoch: 2 [716736/900000 (80%)]\tloss: 0.480483\n","train epoch: 2 [723136/900000 (80%)]\tloss: 0.354958\n","train epoch: 2 [729536/900000 (81%)]\tloss: 0.278473\n","train epoch: 2 [735936/900000 (82%)]\tloss: 0.490307\n","train epoch: 2 [742336/900000 (82%)]\tloss: 0.409817\n","train epoch: 2 [748736/900000 (83%)]\tloss: 0.297772\n","train epoch: 2 [755136/900000 (84%)]\tloss: 0.436628\n","train epoch: 2 [761536/900000 (85%)]\tloss: 0.313015\n","train epoch: 2 [767936/900000 (85%)]\tloss: 0.277762\n","train epoch: 2 [774336/900000 (86%)]\tloss: 0.441139\n","train epoch: 2 [780736/900000 (87%)]\tloss: 0.427079\n","train epoch: 2 [787136/900000 (87%)]\tloss: 0.357635\n","train epoch: 2 [793536/900000 (88%)]\tloss: 0.462678\n","train epoch: 2 [799936/900000 (89%)]\tloss: 0.359206\n","train epoch: 2 [806336/900000 (90%)]\tloss: 0.397068\n","train epoch: 2 [812736/900000 (90%)]\tloss: 0.368924\n","train epoch: 2 [819136/900000 (91%)]\tloss: 0.368293\n","train epoch: 2 [825536/900000 (92%)]\tloss: 0.368441\n","train epoch: 2 [831936/900000 (92%)]\tloss: 0.481114\n","train epoch: 2 [838336/900000 (93%)]\tloss: 0.299332\n","train epoch: 2 [844736/900000 (94%)]\tloss: 0.432043\n","train epoch: 2 [851136/900000 (95%)]\tloss: 0.347000\n","train epoch: 2 [857536/900000 (95%)]\tloss: 0.380593\n","train epoch: 2 [863936/900000 (96%)]\tloss: 0.394503\n","train epoch: 2 [870336/900000 (97%)]\tloss: 0.335766\n","train epoch: 2 [876736/900000 (97%)]\tloss: 0.290307\n","train epoch: 2 [883136/900000 (98%)]\tloss: 0.383327\n","train epoch: 2 [889536/900000 (99%)]\tloss: 0.321479\n","train epoch: 2 [895936/900000 (100%)]\tloss: 0.272907\n","\n","train epoch: 2\taverage loss: 0.383501\taccuracy:82.9224%\n","\n","validation:train epoch: 2\taverage loss: 0.404445\t accuracy:81.90%\n","\n","train epoch: 3 [6336/900000 (1%)]\tloss: 0.257490\n","train epoch: 3 [12736/900000 (1%)]\tloss: 0.243563\n","train epoch: 3 [19136/900000 (2%)]\tloss: 0.310732\n","train epoch: 3 [25536/900000 (3%)]\tloss: 0.376083\n","train epoch: 3 [31936/900000 (4%)]\tloss: 0.304478\n","train epoch: 3 [38336/900000 (4%)]\tloss: 0.430189\n","train epoch: 3 [44736/900000 (5%)]\tloss: 0.455417\n","train epoch: 3 [51136/900000 (6%)]\tloss: 0.364302\n","train epoch: 3 [57536/900000 (6%)]\tloss: 0.329964\n","train epoch: 3 [63936/900000 (7%)]\tloss: 0.285965\n","train epoch: 3 [70336/900000 (8%)]\tloss: 0.323945\n","train epoch: 3 [76736/900000 (9%)]\tloss: 0.418343\n","train epoch: 3 [83136/900000 (9%)]\tloss: 0.394489\n","train epoch: 3 [89536/900000 (10%)]\tloss: 0.385538\n","train epoch: 3 [95936/900000 (11%)]\tloss: 0.309077\n","train epoch: 3 [102336/900000 (11%)]\tloss: 0.413786\n","train epoch: 3 [108736/900000 (12%)]\tloss: 0.357123\n","train epoch: 3 [115136/900000 (13%)]\tloss: 0.282555\n","train epoch: 3 [121536/900000 (14%)]\tloss: 0.253296\n","train epoch: 3 [127936/900000 (14%)]\tloss: 0.272849\n","train epoch: 3 [134336/900000 (15%)]\tloss: 0.273431\n","train epoch: 3 [140736/900000 (16%)]\tloss: 0.305291\n","train epoch: 3 [147136/900000 (16%)]\tloss: 0.310477\n","train epoch: 3 [153536/900000 (17%)]\tloss: 0.300230\n","train epoch: 3 [159936/900000 (18%)]\tloss: 0.319367\n","train epoch: 3 [166336/900000 (18%)]\tloss: 0.402543\n","train epoch: 3 [172736/900000 (19%)]\tloss: 0.245662\n","train epoch: 3 [179136/900000 (20%)]\tloss: 0.321028\n","train epoch: 3 [185536/900000 (21%)]\tloss: 0.301858\n","train epoch: 3 [191936/900000 (21%)]\tloss: 0.332255\n","train epoch: 3 [198336/900000 (22%)]\tloss: 0.591221\n","train epoch: 3 [204736/900000 (23%)]\tloss: 0.552160\n","train epoch: 3 [211136/900000 (23%)]\tloss: 0.346537\n","train epoch: 3 [217536/900000 (24%)]\tloss: 0.551650\n","train epoch: 3 [223936/900000 (25%)]\tloss: 0.355277\n","train epoch: 3 [230336/900000 (26%)]\tloss: 0.320814\n","train epoch: 3 [236736/900000 (26%)]\tloss: 0.235512\n","train epoch: 3 [243136/900000 (27%)]\tloss: 0.294353\n","train epoch: 3 [249536/900000 (28%)]\tloss: 0.237551\n","train epoch: 3 [255936/900000 (28%)]\tloss: 0.307720\n","train epoch: 3 [262336/900000 (29%)]\tloss: 0.532367\n","train epoch: 3 [268736/900000 (30%)]\tloss: 0.396206\n","train epoch: 3 [275136/900000 (31%)]\tloss: 0.315182\n","train epoch: 3 [281536/900000 (31%)]\tloss: 0.305052\n","train epoch: 3 [287936/900000 (32%)]\tloss: 0.450442\n","train epoch: 3 [294336/900000 (33%)]\tloss: 0.221704\n","train epoch: 3 [300736/900000 (33%)]\tloss: 0.298638\n","train epoch: 3 [307136/900000 (34%)]\tloss: 0.319611\n","train epoch: 3 [313536/900000 (35%)]\tloss: 0.224768\n","train epoch: 3 [319936/900000 (36%)]\tloss: 0.288253\n","train epoch: 3 [326336/900000 (36%)]\tloss: 0.465997\n","train epoch: 3 [332736/900000 (37%)]\tloss: 0.448398\n","train epoch: 3 [339136/900000 (38%)]\tloss: 0.350476\n","train epoch: 3 [345536/900000 (38%)]\tloss: 0.275357\n","train epoch: 3 [351936/900000 (39%)]\tloss: 0.222761\n","train epoch: 3 [358336/900000 (40%)]\tloss: 0.203264\n","train epoch: 3 [364736/900000 (41%)]\tloss: 0.427091\n","train epoch: 3 [371136/900000 (41%)]\tloss: 0.342480\n","train epoch: 3 [377536/900000 (42%)]\tloss: 0.325814\n","train epoch: 3 [383936/900000 (43%)]\tloss: 0.311032\n","train epoch: 3 [390336/900000 (43%)]\tloss: 0.208987\n","train epoch: 3 [396736/900000 (44%)]\tloss: 0.336247\n","train epoch: 3 [403136/900000 (45%)]\tloss: 0.321712\n","train epoch: 3 [409536/900000 (46%)]\tloss: 0.362716\n","train epoch: 3 [415936/900000 (46%)]\tloss: 0.200688\n","train epoch: 3 [422336/900000 (47%)]\tloss: 0.447775\n","train epoch: 3 [428736/900000 (48%)]\tloss: 0.308122\n","train epoch: 3 [435136/900000 (48%)]\tloss: 0.253010\n","train epoch: 3 [441536/900000 (49%)]\tloss: 0.306350\n","train epoch: 3 [447936/900000 (50%)]\tloss: 0.327520\n","train epoch: 3 [454336/900000 (50%)]\tloss: 0.217191\n","train epoch: 3 [460736/900000 (51%)]\tloss: 0.313409\n","train epoch: 3 [467136/900000 (52%)]\tloss: 0.247577\n","train epoch: 3 [473536/900000 (53%)]\tloss: 0.415698\n","train epoch: 3 [479936/900000 (53%)]\tloss: 0.435499\n","train epoch: 3 [486336/900000 (54%)]\tloss: 0.320709\n","train epoch: 3 [492736/900000 (55%)]\tloss: 0.236661\n","train epoch: 3 [499136/900000 (55%)]\tloss: 0.439023\n","train epoch: 3 [505536/900000 (56%)]\tloss: 0.347194\n","train epoch: 3 [511936/900000 (57%)]\tloss: 0.263337\n","train epoch: 3 [518336/900000 (58%)]\tloss: 0.355402\n","train epoch: 3 [524736/900000 (58%)]\tloss: 0.533096\n","train epoch: 3 [531136/900000 (59%)]\tloss: 0.266099\n","train epoch: 3 [537536/900000 (60%)]\tloss: 0.379667\n","train epoch: 3 [543936/900000 (60%)]\tloss: 0.275516\n","train epoch: 3 [550336/900000 (61%)]\tloss: 0.309552\n","train epoch: 3 [556736/900000 (62%)]\tloss: 0.252074\n","train epoch: 3 [563136/900000 (63%)]\tloss: 0.323208\n","train epoch: 3 [569536/900000 (63%)]\tloss: 0.278282\n","train epoch: 3 [575936/900000 (64%)]\tloss: 0.448029\n","train epoch: 3 [582336/900000 (65%)]\tloss: 0.406729\n","train epoch: 3 [588736/900000 (65%)]\tloss: 0.499874\n","train epoch: 3 [595136/900000 (66%)]\tloss: 0.313555\n","train epoch: 3 [601536/900000 (67%)]\tloss: 0.310186\n","train epoch: 3 [607936/900000 (68%)]\tloss: 0.339594\n","train epoch: 3 [614336/900000 (68%)]\tloss: 0.381527\n","train epoch: 3 [620736/900000 (69%)]\tloss: 0.506264\n","train epoch: 3 [627136/900000 (70%)]\tloss: 0.411219\n","train epoch: 3 [633536/900000 (70%)]\tloss: 0.390988\n","train epoch: 3 [639936/900000 (71%)]\tloss: 0.315901\n","train epoch: 3 [646336/900000 (72%)]\tloss: 0.421356\n","train epoch: 3 [652736/900000 (73%)]\tloss: 0.346355\n","train epoch: 3 [659136/900000 (73%)]\tloss: 0.285163\n","train epoch: 3 [665536/900000 (74%)]\tloss: 0.335286\n","train epoch: 3 [671936/900000 (75%)]\tloss: 0.321377\n","train epoch: 3 [678336/900000 (75%)]\tloss: 0.381327\n","train epoch: 3 [684736/900000 (76%)]\tloss: 0.302971\n","train epoch: 3 [691136/900000 (77%)]\tloss: 0.229951\n","train epoch: 3 [697536/900000 (78%)]\tloss: 0.338541\n","train epoch: 3 [703936/900000 (78%)]\tloss: 0.384496\n","train epoch: 3 [710336/900000 (79%)]\tloss: 0.341317\n","train epoch: 3 [716736/900000 (80%)]\tloss: 0.294801\n","train epoch: 3 [723136/900000 (80%)]\tloss: 0.271836\n","train epoch: 3 [729536/900000 (81%)]\tloss: 0.394075\n","train epoch: 3 [735936/900000 (82%)]\tloss: 0.360188\n","train epoch: 3 [742336/900000 (82%)]\tloss: 0.231690\n","train epoch: 3 [748736/900000 (83%)]\tloss: 0.298059\n","train epoch: 3 [755136/900000 (84%)]\tloss: 0.518269\n","train epoch: 3 [761536/900000 (85%)]\tloss: 0.494976\n","train epoch: 3 [767936/900000 (85%)]\tloss: 0.301268\n","train epoch: 3 [774336/900000 (86%)]\tloss: 0.445832\n","train epoch: 3 [780736/900000 (87%)]\tloss: 0.264899\n","train epoch: 3 [787136/900000 (87%)]\tloss: 0.342281\n","train epoch: 3 [793536/900000 (88%)]\tloss: 0.339585\n","train epoch: 3 [799936/900000 (89%)]\tloss: 0.241327\n","train epoch: 3 [806336/900000 (90%)]\tloss: 0.245463\n","train epoch: 3 [812736/900000 (90%)]\tloss: 0.451202\n","train epoch: 3 [819136/900000 (91%)]\tloss: 0.285690\n","train epoch: 3 [825536/900000 (92%)]\tloss: 0.295790\n","train epoch: 3 [831936/900000 (92%)]\tloss: 0.437558\n","train epoch: 3 [838336/900000 (93%)]\tloss: 0.326716\n","train epoch: 3 [844736/900000 (94%)]\tloss: 0.418708\n","train epoch: 3 [851136/900000 (95%)]\tloss: 0.352240\n","train epoch: 3 [857536/900000 (95%)]\tloss: 0.483447\n","train epoch: 3 [863936/900000 (96%)]\tloss: 0.331139\n","train epoch: 3 [870336/900000 (97%)]\tloss: 0.365766\n","train epoch: 3 [876736/900000 (97%)]\tloss: 0.202444\n","train epoch: 3 [883136/900000 (98%)]\tloss: 0.247521\n","train epoch: 3 [889536/900000 (99%)]\tloss: 0.406629\n","train epoch: 3 [895936/900000 (100%)]\tloss: 0.332337\n","\n","train epoch: 3\taverage loss: 0.344254\taccuracy:85.0419%\n","\n","validation:train epoch: 3\taverage loss: 0.413093\t accuracy:81.49%\n","\n","train epoch: 4 [6336/900000 (1%)]\tloss: 0.357682\n","train epoch: 4 [12736/900000 (1%)]\tloss: 0.307648\n","train epoch: 4 [19136/900000 (2%)]\tloss: 0.280467\n","train epoch: 4 [25536/900000 (3%)]\tloss: 0.356821\n","train epoch: 4 [31936/900000 (4%)]\tloss: 0.213873\n","train epoch: 4 [38336/900000 (4%)]\tloss: 0.324702\n","train epoch: 4 [44736/900000 (5%)]\tloss: 0.258115\n","train epoch: 4 [51136/900000 (6%)]\tloss: 0.262288\n","train epoch: 4 [57536/900000 (6%)]\tloss: 0.244637\n","train epoch: 4 [63936/900000 (7%)]\tloss: 0.297945\n","train epoch: 4 [70336/900000 (8%)]\tloss: 0.457246\n","train epoch: 4 [76736/900000 (9%)]\tloss: 0.367433\n","train epoch: 4 [83136/900000 (9%)]\tloss: 0.185320\n","train epoch: 4 [89536/900000 (10%)]\tloss: 0.283475\n","train epoch: 4 [95936/900000 (11%)]\tloss: 0.362504\n","train epoch: 4 [102336/900000 (11%)]\tloss: 0.215996\n","train epoch: 4 [108736/900000 (12%)]\tloss: 0.229688\n","train epoch: 4 [115136/900000 (13%)]\tloss: 0.248212\n","train epoch: 4 [121536/900000 (14%)]\tloss: 0.175785\n","train epoch: 4 [127936/900000 (14%)]\tloss: 0.354736\n","train epoch: 4 [134336/900000 (15%)]\tloss: 0.254930\n","train epoch: 4 [140736/900000 (16%)]\tloss: 0.269822\n","train epoch: 4 [147136/900000 (16%)]\tloss: 0.314154\n","train epoch: 4 [153536/900000 (17%)]\tloss: 0.233936\n","train epoch: 4 [159936/900000 (18%)]\tloss: 0.199632\n","train epoch: 4 [166336/900000 (18%)]\tloss: 0.344438\n","train epoch: 4 [172736/900000 (19%)]\tloss: 0.232997\n","train epoch: 4 [179136/900000 (20%)]\tloss: 0.329143\n","train epoch: 4 [185536/900000 (21%)]\tloss: 0.198632\n","train epoch: 4 [191936/900000 (21%)]\tloss: 0.324778\n","train epoch: 4 [198336/900000 (22%)]\tloss: 0.302384\n","train epoch: 4 [204736/900000 (23%)]\tloss: 0.293643\n","train epoch: 4 [211136/900000 (23%)]\tloss: 0.240609\n","train epoch: 4 [217536/900000 (24%)]\tloss: 0.331950\n","train epoch: 4 [223936/900000 (25%)]\tloss: 0.358561\n","train epoch: 4 [230336/900000 (26%)]\tloss: 0.198380\n","train epoch: 4 [236736/900000 (26%)]\tloss: 0.266993\n","train epoch: 4 [243136/900000 (27%)]\tloss: 0.195826\n","train epoch: 4 [249536/900000 (28%)]\tloss: 0.296946\n","train epoch: 4 [255936/900000 (28%)]\tloss: 0.393076\n","train epoch: 4 [262336/900000 (29%)]\tloss: 0.293717\n","train epoch: 4 [268736/900000 (30%)]\tloss: 0.248086\n","train epoch: 4 [275136/900000 (31%)]\tloss: 0.349362\n","train epoch: 4 [281536/900000 (31%)]\tloss: 0.286280\n","train epoch: 4 [287936/900000 (32%)]\tloss: 0.527178\n","train epoch: 4 [294336/900000 (33%)]\tloss: 0.225027\n","train epoch: 4 [300736/900000 (33%)]\tloss: 0.283997\n","train epoch: 4 [307136/900000 (34%)]\tloss: 0.393864\n","train epoch: 4 [313536/900000 (35%)]\tloss: 0.262880\n","train epoch: 4 [319936/900000 (36%)]\tloss: 0.432504\n","train epoch: 4 [326336/900000 (36%)]\tloss: 0.232749\n","train epoch: 4 [332736/900000 (37%)]\tloss: 0.274373\n","train epoch: 4 [339136/900000 (38%)]\tloss: 0.291904\n","train epoch: 4 [345536/900000 (38%)]\tloss: 0.373837\n","train epoch: 4 [351936/900000 (39%)]\tloss: 0.323849\n","train epoch: 4 [358336/900000 (40%)]\tloss: 0.225880\n","train epoch: 4 [364736/900000 (41%)]\tloss: 0.266857\n","train epoch: 4 [371136/900000 (41%)]\tloss: 0.301843\n","train epoch: 4 [377536/900000 (42%)]\tloss: 0.197721\n","train epoch: 4 [383936/900000 (43%)]\tloss: 0.384548\n","train epoch: 4 [390336/900000 (43%)]\tloss: 0.372365\n","train epoch: 4 [396736/900000 (44%)]\tloss: 0.308012\n","train epoch: 4 [403136/900000 (45%)]\tloss: 0.367875\n","train epoch: 4 [409536/900000 (46%)]\tloss: 0.246444\n","train epoch: 4 [415936/900000 (46%)]\tloss: 0.275267\n","train epoch: 4 [422336/900000 (47%)]\tloss: 0.314646\n","train epoch: 4 [428736/900000 (48%)]\tloss: 0.170214\n","train epoch: 4 [435136/900000 (48%)]\tloss: 0.263854\n","train epoch: 4 [441536/900000 (49%)]\tloss: 0.372806\n","train epoch: 4 [447936/900000 (50%)]\tloss: 0.254125\n","train epoch: 4 [454336/900000 (50%)]\tloss: 0.404718\n","train epoch: 4 [460736/900000 (51%)]\tloss: 0.341266\n","train epoch: 4 [467136/900000 (52%)]\tloss: 0.255027\n","train epoch: 4 [473536/900000 (53%)]\tloss: 0.276724\n","train epoch: 4 [479936/900000 (53%)]\tloss: 0.192878\n","train epoch: 4 [486336/900000 (54%)]\tloss: 0.388157\n","train epoch: 4 [492736/900000 (55%)]\tloss: 0.179814\n","train epoch: 4 [499136/900000 (55%)]\tloss: 0.480167\n","train epoch: 4 [505536/900000 (56%)]\tloss: 0.313938\n","train epoch: 4 [511936/900000 (57%)]\tloss: 0.370286\n","train epoch: 4 [518336/900000 (58%)]\tloss: 0.328326\n","train epoch: 4 [524736/900000 (58%)]\tloss: 0.186343\n","train epoch: 4 [531136/900000 (59%)]\tloss: 0.302869\n","train epoch: 4 [537536/900000 (60%)]\tloss: 0.258631\n","train epoch: 4 [543936/900000 (60%)]\tloss: 0.288697\n","train epoch: 4 [550336/900000 (61%)]\tloss: 0.349412\n","train epoch: 4 [556736/900000 (62%)]\tloss: 0.343586\n","train epoch: 4 [563136/900000 (63%)]\tloss: 0.308925\n","train epoch: 4 [569536/900000 (63%)]\tloss: 0.362857\n","train epoch: 4 [575936/900000 (64%)]\tloss: 0.237016\n","train epoch: 4 [582336/900000 (65%)]\tloss: 0.401209\n","train epoch: 4 [588736/900000 (65%)]\tloss: 0.139530\n","train epoch: 4 [595136/900000 (66%)]\tloss: 0.334112\n","train epoch: 4 [601536/900000 (67%)]\tloss: 0.188013\n","train epoch: 4 [607936/900000 (68%)]\tloss: 0.292620\n","train epoch: 4 [614336/900000 (68%)]\tloss: 0.262699\n","train epoch: 4 [620736/900000 (69%)]\tloss: 0.287294\n","train epoch: 4 [627136/900000 (70%)]\tloss: 0.251757\n","train epoch: 4 [633536/900000 (70%)]\tloss: 0.307517\n","train epoch: 4 [639936/900000 (71%)]\tloss: 0.269608\n","train epoch: 4 [646336/900000 (72%)]\tloss: 0.167332\n","train epoch: 4 [652736/900000 (73%)]\tloss: 0.340527\n","train epoch: 4 [659136/900000 (73%)]\tloss: 0.423642\n","train epoch: 4 [665536/900000 (74%)]\tloss: 0.180770\n","train epoch: 4 [671936/900000 (75%)]\tloss: 0.294805\n","train epoch: 4 [678336/900000 (75%)]\tloss: 0.422812\n","train epoch: 4 [684736/900000 (76%)]\tloss: 0.264325\n","train epoch: 4 [691136/900000 (77%)]\tloss: 0.226351\n","train epoch: 4 [697536/900000 (78%)]\tloss: 0.235307\n","train epoch: 4 [703936/900000 (78%)]\tloss: 0.415451\n","train epoch: 4 [710336/900000 (79%)]\tloss: 0.303797\n","train epoch: 4 [716736/900000 (80%)]\tloss: 0.189788\n","train epoch: 4 [723136/900000 (80%)]\tloss: 0.239976\n","train epoch: 4 [729536/900000 (81%)]\tloss: 0.329726\n","train epoch: 4 [735936/900000 (82%)]\tloss: 0.301927\n","train epoch: 4 [742336/900000 (82%)]\tloss: 0.329041\n","train epoch: 4 [748736/900000 (83%)]\tloss: 0.405935\n","train epoch: 4 [755136/900000 (84%)]\tloss: 0.235068\n","train epoch: 4 [761536/900000 (85%)]\tloss: 0.239633\n","train epoch: 4 [767936/900000 (85%)]\tloss: 0.228777\n","train epoch: 4 [774336/900000 (86%)]\tloss: 0.339275\n","train epoch: 4 [780736/900000 (87%)]\tloss: 0.335615\n","train epoch: 4 [787136/900000 (87%)]\tloss: 0.344538\n","train epoch: 4 [793536/900000 (88%)]\tloss: 0.340460\n","train epoch: 4 [799936/900000 (89%)]\tloss: 0.264337\n","train epoch: 4 [806336/900000 (90%)]\tloss: 0.289229\n","train epoch: 4 [812736/900000 (90%)]\tloss: 0.331339\n","train epoch: 4 [819136/900000 (91%)]\tloss: 0.326091\n","train epoch: 4 [825536/900000 (92%)]\tloss: 0.337355\n","train epoch: 4 [831936/900000 (92%)]\tloss: 0.310458\n","train epoch: 4 [838336/900000 (93%)]\tloss: 0.297702\n","train epoch: 4 [844736/900000 (94%)]\tloss: 0.230120\n","train epoch: 4 [851136/900000 (95%)]\tloss: 0.264516\n","train epoch: 4 [857536/900000 (95%)]\tloss: 0.252819\n","train epoch: 4 [863936/900000 (96%)]\tloss: 0.246521\n","train epoch: 4 [870336/900000 (97%)]\tloss: 0.353791\n","train epoch: 4 [876736/900000 (97%)]\tloss: 0.200734\n","train epoch: 4 [883136/900000 (98%)]\tloss: 0.248129\n","train epoch: 4 [889536/900000 (99%)]\tloss: 0.286953\n","train epoch: 4 [895936/900000 (100%)]\tloss: 0.325802\n","\n","train epoch: 4\taverage loss: 0.304907\taccuracy:87.0583%\n","\n","validation:train epoch: 4\taverage loss: 0.424910\t accuracy:81.67%\n","\n","train epoch: 5 [6336/900000 (1%)]\tloss: 0.228717\n","train epoch: 5 [12736/900000 (1%)]\tloss: 0.224289\n","train epoch: 5 [19136/900000 (2%)]\tloss: 0.252708\n","train epoch: 5 [25536/900000 (3%)]\tloss: 0.303940\n","train epoch: 5 [31936/900000 (4%)]\tloss: 0.181497\n","train epoch: 5 [38336/900000 (4%)]\tloss: 0.289691\n","train epoch: 5 [44736/900000 (5%)]\tloss: 0.237015\n","train epoch: 5 [51136/900000 (6%)]\tloss: 0.170821\n","train epoch: 5 [57536/900000 (6%)]\tloss: 0.354481\n","train epoch: 5 [63936/900000 (7%)]\tloss: 0.158908\n","train epoch: 5 [70336/900000 (8%)]\tloss: 0.283943\n","train epoch: 5 [76736/900000 (9%)]\tloss: 0.255970\n","train epoch: 5 [83136/900000 (9%)]\tloss: 0.330329\n","train epoch: 5 [89536/900000 (10%)]\tloss: 0.212267\n","train epoch: 5 [95936/900000 (11%)]\tloss: 0.240082\n","train epoch: 5 [102336/900000 (11%)]\tloss: 0.181308\n","train epoch: 5 [108736/900000 (12%)]\tloss: 0.178717\n","train epoch: 5 [115136/900000 (13%)]\tloss: 0.358309\n","train epoch: 5 [121536/900000 (14%)]\tloss: 0.344329\n","train epoch: 5 [127936/900000 (14%)]\tloss: 0.158245\n","train epoch: 5 [134336/900000 (15%)]\tloss: 0.137281\n","train epoch: 5 [140736/900000 (16%)]\tloss: 0.273109\n","train epoch: 5 [147136/900000 (16%)]\tloss: 0.301373\n","train epoch: 5 [153536/900000 (17%)]\tloss: 0.168324\n","train epoch: 5 [159936/900000 (18%)]\tloss: 0.371305\n","train epoch: 5 [166336/900000 (18%)]\tloss: 0.316923\n","train epoch: 5 [172736/900000 (19%)]\tloss: 0.154965\n","train epoch: 5 [179136/900000 (20%)]\tloss: 0.193228\n","train epoch: 5 [185536/900000 (21%)]\tloss: 0.173261\n","train epoch: 5 [191936/900000 (21%)]\tloss: 0.300070\n","train epoch: 5 [198336/900000 (22%)]\tloss: 0.336518\n","train epoch: 5 [204736/900000 (23%)]\tloss: 0.208051\n","train epoch: 5 [211136/900000 (23%)]\tloss: 0.214938\n","train epoch: 5 [217536/900000 (24%)]\tloss: 0.295536\n","train epoch: 5 [223936/900000 (25%)]\tloss: 0.223907\n","train epoch: 5 [230336/900000 (26%)]\tloss: 0.273527\n","train epoch: 5 [236736/900000 (26%)]\tloss: 0.260677\n","train epoch: 5 [243136/900000 (27%)]\tloss: 0.164878\n","train epoch: 5 [249536/900000 (28%)]\tloss: 0.261086\n","train epoch: 5 [255936/900000 (28%)]\tloss: 0.226862\n","train epoch: 5 [262336/900000 (29%)]\tloss: 0.364972\n","train epoch: 5 [268736/900000 (30%)]\tloss: 0.324594\n","train epoch: 5 [275136/900000 (31%)]\tloss: 0.204194\n","train epoch: 5 [281536/900000 (31%)]\tloss: 0.275488\n","train epoch: 5 [287936/900000 (32%)]\tloss: 0.216554\n","train epoch: 5 [294336/900000 (33%)]\tloss: 0.233017\n","train epoch: 5 [300736/900000 (33%)]\tloss: 0.256313\n","train epoch: 5 [307136/900000 (34%)]\tloss: 0.195936\n","train epoch: 5 [313536/900000 (35%)]\tloss: 0.319476\n","train epoch: 5 [319936/900000 (36%)]\tloss: 0.286788\n","train epoch: 5 [326336/900000 (36%)]\tloss: 0.339680\n","train epoch: 5 [332736/900000 (37%)]\tloss: 0.291314\n","train epoch: 5 [339136/900000 (38%)]\tloss: 0.387568\n","train epoch: 5 [345536/900000 (38%)]\tloss: 0.187014\n","train epoch: 5 [351936/900000 (39%)]\tloss: 0.404891\n","train epoch: 5 [358336/900000 (40%)]\tloss: 0.127371\n","train epoch: 5 [364736/900000 (41%)]\tloss: 0.272472\n","train epoch: 5 [371136/900000 (41%)]\tloss: 0.303617\n","train epoch: 5 [377536/900000 (42%)]\tloss: 0.273246\n","train epoch: 5 [383936/900000 (43%)]\tloss: 0.207195\n","train epoch: 5 [390336/900000 (43%)]\tloss: 0.207629\n","train epoch: 5 [396736/900000 (44%)]\tloss: 0.284555\n","train epoch: 5 [403136/900000 (45%)]\tloss: 0.221961\n","train epoch: 5 [409536/900000 (46%)]\tloss: 0.292255\n","train epoch: 5 [415936/900000 (46%)]\tloss: 0.278325\n","train epoch: 5 [422336/900000 (47%)]\tloss: 0.375271\n","train epoch: 5 [428736/900000 (48%)]\tloss: 0.289398\n","train epoch: 5 [435136/900000 (48%)]\tloss: 0.172311\n","train epoch: 5 [441536/900000 (49%)]\tloss: 0.331545\n","train epoch: 5 [447936/900000 (50%)]\tloss: 0.140655\n","train epoch: 5 [454336/900000 (50%)]\tloss: 0.231965\n","train epoch: 5 [460736/900000 (51%)]\tloss: 0.325250\n","train epoch: 5 [467136/900000 (52%)]\tloss: 0.264984\n","train epoch: 5 [473536/900000 (53%)]\tloss: 0.254162\n","train epoch: 5 [479936/900000 (53%)]\tloss: 0.212082\n","train epoch: 5 [486336/900000 (54%)]\tloss: 0.223112\n","train epoch: 5 [492736/900000 (55%)]\tloss: 0.205966\n","train epoch: 5 [499136/900000 (55%)]\tloss: 0.291273\n","train epoch: 5 [505536/900000 (56%)]\tloss: 0.308891\n","train epoch: 5 [511936/900000 (57%)]\tloss: 0.234252\n","train epoch: 5 [518336/900000 (58%)]\tloss: 0.176343\n","train epoch: 5 [524736/900000 (58%)]\tloss: 0.397162\n","train epoch: 5 [531136/900000 (59%)]\tloss: 0.310731\n","train epoch: 5 [537536/900000 (60%)]\tloss: 0.310462\n","train epoch: 5 [543936/900000 (60%)]\tloss: 0.279876\n","train epoch: 5 [550336/900000 (61%)]\tloss: 0.216238\n","train epoch: 5 [556736/900000 (62%)]\tloss: 0.248422\n","train epoch: 5 [563136/900000 (63%)]\tloss: 0.238427\n","train epoch: 5 [569536/900000 (63%)]\tloss: 0.267038\n","train epoch: 5 [575936/900000 (64%)]\tloss: 0.160302\n","train epoch: 5 [582336/900000 (65%)]\tloss: 0.302555\n","train epoch: 5 [588736/900000 (65%)]\tloss: 0.184227\n","train epoch: 5 [595136/900000 (66%)]\tloss: 0.212244\n","train epoch: 5 [601536/900000 (67%)]\tloss: 0.236146\n","train epoch: 5 [607936/900000 (68%)]\tloss: 0.276815\n","train epoch: 5 [614336/900000 (68%)]\tloss: 0.215516\n","train epoch: 5 [620736/900000 (69%)]\tloss: 0.258551\n","train epoch: 5 [627136/900000 (70%)]\tloss: 0.232925\n","train epoch: 5 [633536/900000 (70%)]\tloss: 0.302607\n","train epoch: 5 [639936/900000 (71%)]\tloss: 0.192582\n","train epoch: 5 [646336/900000 (72%)]\tloss: 0.166502\n","train epoch: 5 [652736/900000 (73%)]\tloss: 0.193452\n","train epoch: 5 [659136/900000 (73%)]\tloss: 0.488276\n","train epoch: 5 [665536/900000 (74%)]\tloss: 0.156108\n","train epoch: 5 [671936/900000 (75%)]\tloss: 0.192084\n","train epoch: 5 [678336/900000 (75%)]\tloss: 0.293673\n","train epoch: 5 [684736/900000 (76%)]\tloss: 0.268854\n","train epoch: 5 [691136/900000 (77%)]\tloss: 0.248857\n","train epoch: 5 [697536/900000 (78%)]\tloss: 0.189108\n","train epoch: 5 [703936/900000 (78%)]\tloss: 0.189993\n","train epoch: 5 [710336/900000 (79%)]\tloss: 0.220141\n","train epoch: 5 [716736/900000 (80%)]\tloss: 0.242057\n","train epoch: 5 [723136/900000 (80%)]\tloss: 0.206697\n","train epoch: 5 [729536/900000 (81%)]\tloss: 0.251093\n","train epoch: 5 [735936/900000 (82%)]\tloss: 0.311818\n","train epoch: 5 [742336/900000 (82%)]\tloss: 0.609991\n","train epoch: 5 [748736/900000 (83%)]\tloss: 0.204289\n","train epoch: 5 [755136/900000 (84%)]\tloss: 0.256869\n","train epoch: 5 [761536/900000 (85%)]\tloss: 0.305638\n","train epoch: 5 [767936/900000 (85%)]\tloss: 0.381870\n","train epoch: 5 [774336/900000 (86%)]\tloss: 0.362624\n","train epoch: 5 [780736/900000 (87%)]\tloss: 0.179685\n","train epoch: 5 [787136/900000 (87%)]\tloss: 0.153559\n","train epoch: 5 [793536/900000 (88%)]\tloss: 0.265597\n","train epoch: 5 [799936/900000 (89%)]\tloss: 0.240837\n","train epoch: 5 [806336/900000 (90%)]\tloss: 0.275367\n","train epoch: 5 [812736/900000 (90%)]\tloss: 0.222263\n","train epoch: 5 [819136/900000 (91%)]\tloss: 0.370406\n","train epoch: 5 [825536/900000 (92%)]\tloss: 0.203613\n","train epoch: 5 [831936/900000 (92%)]\tloss: 0.245924\n","train epoch: 5 [838336/900000 (93%)]\tloss: 0.183968\n","train epoch: 5 [844736/900000 (94%)]\tloss: 0.236999\n","train epoch: 5 [851136/900000 (95%)]\tloss: 0.217216\n","train epoch: 5 [857536/900000 (95%)]\tloss: 0.238257\n","train epoch: 5 [863936/900000 (96%)]\tloss: 0.273063\n","train epoch: 5 [870336/900000 (97%)]\tloss: 0.194000\n","train epoch: 5 [876736/900000 (97%)]\tloss: 0.335903\n","train epoch: 5 [883136/900000 (98%)]\tloss: 0.214372\n","train epoch: 5 [889536/900000 (99%)]\tloss: 0.289123\n","train epoch: 5 [895936/900000 (100%)]\tloss: 0.205724\n","\n","train epoch: 5\taverage loss: 0.264465\taccuracy:88.9927%\n","\n","validation:train epoch: 5\taverage loss: 0.460834\t accuracy:81.07%\n","\n"]}]}]}