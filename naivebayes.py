# -*- coding: utf-8 -*-
"""NaiveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13hFHk6IvqQV7_Z6wcairbz8tvY2FvYnw
"""

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sn
import matplotlib.pyplot as plt
import contractions
import nltk
import re
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB
from sklearn import metrics


from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""https://www.analyticsvidhya.com/blog/2021/09/sentiment-classification-using-nlp-with-text-analytics/

# get data
"""

def get_detaset():
    train_df = pd.read_csv('/content/drive/MyDrive/IFT6390/kaggle2/train_data.csv')
    test_df = pd.read_csv('/content/drive/MyDrive/IFT6390/kaggle2/test_data.csv')
    train_result_df = pd.read_csv('/content/drive/MyDrive/IFT6390/kaggle2/train_results.csv')
    return train_df, test_df, train_result_df

def treat_detaset(train_df, test_df, train_result_df):
    train_df= train_df.drop(columns=['id'])
    test_df = test_df.drop(columns=['id'])
    train_result_df = train_result_df.drop(columns=['id'])

    train_result_df.loc[train_result_df['target'] == 'negative'] = 0
    train_result_df.loc[train_result_df['target'] == 'neutral'] = 1
    train_result_df.loc[train_result_df['target'] == 'positive'] = 2
    return train_df, test_df, train_result_df

train_df, test_df, train_result_df = get_detaset()
train_df, test_df, train_result_df = treat_detaset(train_df, test_df, train_result_df)

"""# text preprocessing

https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/?
"""

#def stopwords
nltk.download('stopwords')
stop_words_list = stopwords.words('english')
stop_words = set(stop_words_list[:88])
#Only delete part of stopwords, keep not, no... that may affect emotion
stop_words.add('subject')
stop_words.add('http')

nltk.download("wordnet")
nltk.download('omw-1.4')
lemmatizer = WordNetLemmatizer()

def text_preprocessing(df):
    #1) Expand contractions in Text Processing
    df['reviews_text']=df['text'].apply(lambda x:contractions.fix(x, slang=True))
    #2) Lower Case
    df['reviews_text'] = df['reviews_text'].str.lower()
    #3) Remove punctuations
    df['reviews_text'] = df['reviews_text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))
    #4) Remove words containing digits
    df['reviews_text'] = df['reviews_text'].apply(lambda x: re.sub(r'\w*\d\w*', '', x))
    #5) Remove Stopwords
    def remove_stopwords(text):
        return " ".join([word for word in str(text).split() if word not in stop_words])
    df['reviews_text'] = df['reviews_text'].apply(lambda x: remove_stopwords(x))
    #6) Lemmatization
    def lemmatize_words(text):
        return " ".join([lemmatizer.lemmatize(word) for word in text.split()])
    df['reviews_text'] = df['reviews_text'].apply(lambda text: lemmatize_words(text))
    #7) Remove Extra Spaces
    df['reviews_text'] = df['reviews_text'].apply(lambda x: re.sub(' +', ' ', x))
    return df

train_df_pre = text_preprocessing(train_df)
test_df_pre = text_preprocessing(test_df)

"""# **Creating Count Vectors (bag of words) for Dataset**

https://www.analyticsvidhya.com/blog/2021/08/text-preprocessing-techniques-for-performing-sentiment-analysis/#h2_3
"""

count_vectorize = CountVectorizer(max_features = 1000, min_df = 20)

def creating_count_vectors(df):
    feature_vector =  count_vectorize.fit(df.reviews_text)
    features = feature_vector.get_feature_names()
    ds_features = count_vectorize.transform(df.reviews_text)
    return features, ds_features

#get features(BOW) and transform data to BOW
 #features, train_ds_features = creating_count_vectors(train_df_pre)

train_ds_features = count_vectorize.fit_transform(train_df_pre.reviews_text)
test_ds_features = count_vectorize.transform(test_df_pre.reviews_text)

"""# **Build Model for sentiment classification**"""

#splid train valid
train_x, valid_x, train_y, valid_y =  train_test_split(train_ds_features, train_result_df.target, 
                                                     test_size = 0.2, random_state = 42)

#train and valid
nb_clf = MultinomialNB()
nb_clf.fit(train_x.toarray(), train_y.astype(int))
test_ds_predicted = nb_clf.predict(valid_x.toarray())
print(metrics.classification_report(valid_y.astype(int),test_ds_predicted))
cm = metrics.confusion_matrix(valid_y.astype(int), test_ds_predicted)
sn.heatmap(cm, annot=True, fmt = '.2f')
test_predicted = nb_clf.predict(test_ds_features.toarray())

df_submit = pd.DataFrame(test_predicted, columns = ['target'])
df_submit = df_submit.reset_index()
df_submit.rename(columns={'index':'id'}, inplace = True)
df_submit.to_csv('./bayes3.csv', index = False)

"""# **Creating Count Vectors (bag of words) for Dataset**
# + TF-IDF

"""

count_vectorize_T = TfidfVectorizer(max_features = 1000, min_df = 20)

train_ds_features_T = count_vectorize_T.fit_transform(train_df_pre.reviews_text)
test_ds_features_T = count_vectorize_T.transform(test_df_pre.reviews_text)

#splid train valid
train_x, valid_x, train_y, valid_y =  train_test_split(train_ds_features_T, train_result_df.target, 
                                                     test_size = 0.2, random_state = 42)

#train and valid
nb_clf = MultinomialNB()
nb_clf.fit(train_x.toarray(), train_y.astype(int))
test_ds_predicted = nb_clf.predict(valid_x.toarray())
print(metrics.classification_report(valid_y.astype(int),test_ds_predicted))
cm = metrics.confusion_matrix(valid_y.astype(int), test_ds_predicted)
sn.heatmap(cm, annot=True, fmt = '.2f')